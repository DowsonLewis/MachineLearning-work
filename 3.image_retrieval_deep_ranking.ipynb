{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/dl_banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oj3JfEYwEAH8"
   },
   "source": [
    "# 深度相似度排序模型\n",
    "\n",
    "**提示：如果大家觉得计算资源有限，欢迎大家在翻-墙后免费试用[google的colab](https://colab.research.google.com)，有免费的K80 GPU供大家使用，大家只需要把课程的notebook上传即可运行**\n",
    "\n",
    "#### \\[稀牛学院 x 网易云课程\\]《深度学习工程师(实战)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)\n",
    "当我们构建一个图像检索系统的时候，我们很核心的一个问题是，如何评估图像相似度。我们上一个notebook讲到的方式，是通过用作分类的卷积神经网络抽取图像特征，比对图像特征的距离来判断图像是否相似。\n",
    "\n",
    "上面提到的方法当然可以完成检索任务，但是一方面这个检索过程并不是一个端到端学习出来的结果；另外一方面两张图片是否相似是一个主观性很强的东西，有一些图片，在一部分看来是相似的，在另外一部分人看来并不那么相似，我们有没有办法让神经网络通过观察标注好的相似与不相似的图片，去以数据驱动的方式总结出来如何评估相似呢？这就是我们提到的deep ranking模型，这种模型会以三元组形态的训练样本来学习细粒度的图像相似性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6epkbMZFvfD"
   },
   "source": [
    "## 三元组训练集\n",
    "#### \\[稀牛学院 x 网易云课程\\]《深度学习工程师(实战)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)\n",
    "那么什么是三元组呢，三元组包含查询图像，正图像和负图像，其中正图像与查询图像比负图像更相似。下图说明了这一点：\n",
    "![](../img/triplet.png)\n",
    "在上图中，每列是一个三元组样本。上中下行对应于查询图像，正图像和负图像，其中正图像相比于负图像和查询图像更相似。\n",
    "\n",
    "三元组样本可以帮助神经网络学习到相似性排序，也就是图A和图B，现对于图C和图B更接近一些。深度排序模型可以使用这种细粒度图像相似性信息，这种相对性信息在类别级别图像相似性模型或分类模型中没有办法学习到，深度排序模型在不少场景下有更贴近场景的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbAH0MXKG_hv"
   },
   "source": [
    "## 图像相似度学习vs图像分类\n",
    "#### \\[稀牛学院 x 网易云课程\\]《深度学习工程师(实战)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)\n",
    "考虑停放在道路上的三辆汽车：一辆黑色轿车，一辆白色轿车和一辆深灰色汽车。对于图像分类模型，这三款车都只是汽车。它不关心汽车的颜色和其他方面。对于类似的图像深度排序模型，它也会查看汽车的颜色和其他方面。如果查询图像是“黑色汽车”，则类似图像排序模型将“深灰色汽车”排列为高于“白色汽车”。因此，图像分类模型可能不直接适合于区分细粒度图像相似性的任务。[深度排序论文](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42945.pdf)的作者提出使用深度排名模型来学习细粒度图像相似性，该模型表征与一组三元组的细粒度图像相似性关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zs5Eup3dGF1W"
   },
   "source": [
    "## 如何测量两幅图像的相似度？\n",
    "#### \\[稀牛学院 x 网易云课程\\]《深度学习工程师(实战)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)\n",
    "用于测量图像之间相似性的度量可能是构建图像相似度模型中最重要的因素。虽然可以使用不同的度量来定义相似性，但最常用的是L1范数（也称为曼哈顿距离）和L2范数（也称为欧几里德距离）。深度排序模型在图像相似性的背景下，给出了一个非常好的解释和曼哈顿与欧几里德距离之间的比较。他们的结果表明，在我们使用的那种自然图像领域，曼哈顿距离可以更好地捕捉人类的图像相似性概念。在深度排名的情况下，作者使用Squared Euclidean距离作为相似度量。距离越小，图像越相似。\n",
    "\n",
    "## 关于损失函数的构建\n",
    "设计神经网络一个很重要的环节是构建合理的损失函数，回到任务本身，如果得到一个合理的评估相似度的深度学习模型，我们需要保证的事情是如下的公式：\n",
    "$$\\begin{array} { l } { D \\left( f \\left( p _ { i } \\right) , f \\left( p _ { i } ^ { + } \\right) \\right) < D \\left( f \\left( p _ { i } \\right) , f \\left( p _ { i } ^ { - } \\right) \\right) } \\\\ { \\forall p _ { i } , p _ { i } ^ { + } , p _ { i } ^ { - } \\text { such that } r \\left( p _ { i } , p _ { i } ^ { + } \\right) > r \\left( p _ { i } , p _ { i } ^ { - } \\right) } \\end{array}$$\n",
    "\n",
    "其中'f'是将图像映射到矢量的嵌入函数。$P_i$是查询图像，$P_i^+$是正图像，$P_i^-$是负图像，'r'是两个图像之间的相似距离。\n",
    "\n",
    "参考大家在SVM中学习到的合页损失，我们可以把损失函数设计成如下的形式：\n",
    "$$l \\left( p _ { i } , p _ { i } ^ { + } , p _ { i } ^ { - } \\right) = \\max \\left\\{ 0 , g + D \\left( f \\left( p _ { i } \\right) , f \\left( p _ { i } ^ { + } \\right) \\right) - D \\left( f \\left( p _ { i } \\right) , f \\left( p _ { i } ^ { - } \\right) \\right) \\right\\}$$\n",
    "\n",
    "其中'l'是三元组的合页损失，'g'是一个间隔超参数，它规定了两个图像对的距离之间的差距：$(P_i, P_i^+)$和$(P_i, P_i^-)$，'D'是两个欧几里德点之间的欧氏距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RvUNMWR-Ai4"
   },
   "source": [
    "## 模型构建\n",
    "#### \\[稀牛学院 x 网易云课程\\]《深度学习工程师(实战)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)\n",
    "我们把整个网络设计为如下的形态\n",
    "![](../img/network_structure.png)\n",
    "\n",
    "该网络由3部分组成：\n",
    "- 三元组采样\n",
    "- ConvNet\n",
    "- 图像相似度排序\n",
    "\n",
    "网络接受图像三元组作为输入。一个图像三元组包含查询图像$p_i$，正图像$p_i^+$和负图像$p_i^-$，它们被独立地送入到具有共享架构和参数的三个相同的深度卷积神经网络 $f(p)$中。三元组表征三个图像的相对相似关系。深度神经网络$f(p)$计算图像$p_i$的嵌入：$f(p_i)∈R_d$，其中$d$是特征嵌入的维度，$R$表示实数空间。\n",
    "\n",
    "顶部的排序层计算三元组的合页损失。学习过程中，不断使用loss反向传播得到的梯度调整参数，减小损失。\n",
    "\n",
    "上图中的ConvNet是一种新颖的多尺度深度神经网络结构，具体结构如下：\n",
    "![](../img/CNN_structure.png)\n",
    "\n",
    "其中上端的ConvNet是一个典型的深度卷积神经网络，抽取大部分的图像语义信息，下方的两个部分对图像下采样后使用更浅的神经网络结构用于捕捉细节纹理特征(大家指导卷积神经网络的浅层卷积层捕捉的特征会更底层) ，我们讲神经网络捕捉到的不同粒度的信息通过线性嵌入层进行组合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WPrk_sdBkQD"
   },
   "source": [
    "## 三元组样本的选择\n",
    "#### \\[稀牛学院 x 网易云课程\\]《深度学习工程师(实战)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)\n",
    "另外一个很重要的环节是构建样本，这里的三元组样本的质与量会直接影响模型学习的好坏，我们知道总体说来，我们要从相似的图片中选出一个pair，再从不相似的图片中抽取一张作为负样本。\n",
    "\n",
    "![](../img/sampling.png)\n",
    "\n",
    "注意，如果想让模型学习得更好，大家在构建负样本的时候，不仅要随机选取不同类别(图像内容不同)的图片，还要选择更有挑战一些的图片(内容有一定相似性，但是人判断相对于正样本更不相似的图片)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "vFPNmFNBtDpO",
    "outputId": "48cb45d7-9c13-4084-c171-49e959858549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 591.8MB 30kB/s \n",
      "tcmalloc: large alloc 1073750016 bytes == 0x60d06000 @  0x7f7d2e21c2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
      "\u001b[?25hCollecting torchvision\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 24.2MB/s \n",
      "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/5e/e91792f198bbc5a0d7d3055ad552bc4062942d27eaf75c3e2783cf64eae5/Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.0MB 3.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
      "Installing collected packages: torch, pillow, torchvision\n",
      "  Found existing installation: Pillow 4.0.0\n",
      "    Uninstalling Pillow-4.0.0:\n",
      "      Successfully uninstalled Pillow-4.0.0\n",
      "Successfully installed pillow-5.4.1 torch-1.0.0 torchvision-0.2.1\n",
      "--2019-01-13 12:11:13--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
      "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
      "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248100043 (237M) [application/zip]\n",
      "Saving to: ‘tiny-imagenet-200.zip’\n",
      "\n",
      "tiny-imagenet-200.z 100%[===================>] 236.61M  9.34MB/s    in 25s     \n",
      "\n",
      "2019-01-13 12:11:39 (9.42 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -rf tiny*\n",
    "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1BP6xejMtwBS"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zfile = zipfile.ZipFile('tiny-imagenet-200.zip','r')\n",
    "zfile.extractall()\n",
    "zfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tEGxfTObtw1T"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# 递归遍历文件夹，并以一定的几率把图像名写入文件\n",
    "def gci(filepath, outpath):\n",
    "  #遍历filepath下所有文件，包括子目录\n",
    "  files = os.listdir(filepath)\n",
    "  for fi in files:\n",
    "    fi_d = os.path.join(filepath,fi)            \n",
    "    if os.path.isdir(fi_d):\n",
    "      gci(fi_d, outpath)                  \n",
    "    else:\n",
    "      if random.random()<=0.05 and fi_d.endswith(\".JPEG\"):\n",
    "        name_parts = os.path.join(fi_d).split(\"/\")\n",
    "        directory = outpath+\"/\"+name_parts[2]\n",
    "        if not os.path.exists(directory):\n",
    "          os.makedirs(directory)\n",
    "        shutil.copy2(os.path.join(fi_d), directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYDHyjX2HTkJ"
   },
   "outputs": [],
   "source": [
    "filepath = \"tiny-imagenet-200\"\n",
    "outpath = \"triplet\"\n",
    "gci(filepath, outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "XNT3YlrAIllX",
    "outputId": "1c7a2180-ca72-413b-fc71-185800a5fc5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n02963159_114.JPEG  n02963159_293.JPEG\tn02963159_394.JPEG  n02963159_446.JPEG\n",
      "n02963159_121.JPEG  n02963159_297.JPEG\tn02963159_39.JPEG   n02963159_466.JPEG\n",
      "n02963159_136.JPEG  n02963159_304.JPEG\tn02963159_3.JPEG    n02963159_467.JPEG\n",
      "n02963159_158.JPEG  n02963159_318.JPEG\tn02963159_402.JPEG  n02963159_480.JPEG\n",
      "n02963159_177.JPEG  n02963159_326.JPEG\tn02963159_418.JPEG  n02963159_49.JPEG\n",
      "n02963159_226.JPEG  n02963159_33.JPEG\tn02963159_420.JPEG  n02963159_52.JPEG\n",
      "n02963159_233.JPEG  n02963159_352.JPEG\tn02963159_439.JPEG  n02963159_58.JPEG\n",
      "n02963159_234.JPEG  n02963159_371.JPEG\tn02963159_43.JPEG   n02963159_5.JPEG\n",
      "n02963159_279.JPEG  n02963159_388.JPEG\tn02963159_440.JPEG\n"
     ]
    }
   ],
   "source": [
    "!ls triplet/n02963159"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZNfqkEyDO4M"
   },
   "source": [
    "## 三元组采样\n",
    "#### \\[稀牛学院 x 网易云课程\\]《深度学习工程师(实战)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)\n",
    "\n",
    "我们从形如下面的文件组织形式中产出三元组用于训练\n",
    "```\n",
    "dataset/\n",
    "|__SimilarityClass1/\n",
    "|    |__Image1.jpg, Image2.jpg and so on....\n",
    "|\n",
    "|__SimilarityClass2/\n",
    "|    |_Image1.jpg, Image2.jpg and so on....\n",
    "|\n",
    "and so on...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdxUxeFpDLFx"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "def list_pictures(directory, ext='JPEG|jpg|jpeg|bmp|png|ppm'):\n",
    "    return [os.path.join(root, f)\n",
    "            for root, _, files in os.walk(directory) for f in files\n",
    "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f)]\n",
    "\n",
    "\n",
    "def get_negative_images(all_images,image_names,num_neg_images):\n",
    "    random_numbers = np.arange(len(all_images))\n",
    "    np.random.shuffle(random_numbers)\n",
    "    if int(num_neg_images)>(len(all_images)-1):\n",
    "        num_neg_images = len(all_images)-1\n",
    "    neg_count = 0\n",
    "    negative_images = []\n",
    "    for random_number in list(random_numbers):\n",
    "        if all_images[random_number] not in image_names:\n",
    "            negative_images.append(all_images[random_number])\n",
    "            neg_count += 1\n",
    "            if neg_count>(int(num_neg_images)-1):\n",
    "                break\n",
    "    return negative_images\n",
    "\n",
    "def get_positive_images(image_name,image_names,num_pos_images):\n",
    "    random_numbers = np.arange(len(image_names))\n",
    "    np.random.shuffle(random_numbers)\n",
    "    if int(num_pos_images)>(len(image_names)-1):\n",
    "        num_pos_images = len(image_names)-1\n",
    "    pos_count = 0\n",
    "    positive_images = []\n",
    "    for random_number in list(random_numbers):\n",
    "        if image_names[random_number]!= image_name:\n",
    "            positive_images.append(image_names[random_number])\n",
    "            pos_count += 1 \n",
    "            if int(pos_count)>(int(num_pos_images)-1):\n",
    "                break\n",
    "    return positive_images\n",
    "\n",
    "def triplet_sampler(directory_path, output_path,num_neg_images,num_pos_images):\n",
    "    classes = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
    "    all_images = []\n",
    "    for class_ in classes:\n",
    "        all_images += (list_pictures(os.path.join(directory_path,class_)))\n",
    "    triplets = []\n",
    "    for class_ in classes:\n",
    "        image_names = list_pictures(os.path.join(directory_path,class_))\n",
    "        for image_name in image_names:\n",
    "            image_names_set = set(image_names)\n",
    "            query_image = image_name\n",
    "            positive_images = get_positive_images(image_name,image_names,num_pos_images)\n",
    "            for positive_image in positive_images:\n",
    "                negative_images = get_negative_images(all_images,set(image_names),num_neg_images)\n",
    "                for negative_image in negative_images:\n",
    "                    triplets.append(query_image+',')\n",
    "                    triplets.append(positive_image+',')\n",
    "                    triplets.append(negative_image+'\\n')\n",
    "            \n",
    "    f = open(os.path.join(output_path,\"triplets.txt\"),'w')\n",
    "    f.write(\"\".join(triplets))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPpBewrfJIai"
   },
   "outputs": [],
   "source": [
    "input_dir = 'triplet'\n",
    "output_dir = 'train_samples'\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)\n",
    "num_neg_images = 10\n",
    "num_pos_images = 10\n",
    "triplet_sampler(directory_path=input_dir, output_path=output_dir, num_neg_images=num_neg_images, num_pos_images=num_pos_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "8IDL3im2JZU3",
    "outputId": "33797494-6254-4f47-8f6f-220cbea4ab50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triplet/n02843684/n02843684_308.JPEG,triplet/n02843684/n02843684_230.JPEG,triplet/n02226429/n02226429_20.JPEG\n",
      "triplet/n02843684/n02843684_308.JPEG,triplet/n02843684/n02843684_230.JPEG,triplet/n07720875/n07720875_376.JPEG\n",
      "triplet/n02843684/n02843684_308.JPEG,triplet/n02843684/n02843684_230.JPEG,triplet/n02917067/n02917067_0.JPEG\n",
      "triplet/n02843684/n02843684_308.JPEG,triplet/n02843684/n02843684_230.JPEG,triplet/n04265275/n04265275_44.JPEG\n",
      "triplet/n02843684/n02843684_308.JPEG,triplet/n02843684/n02843684_230.JPEG,triplet/images/val_7166.JPEG\n"
     ]
    }
   ],
   "source": [
    "!head -5 train_samples/triplets.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w7TOP5L4KEgo",
    "outputId": "e454bf26-bafc-4713-9581-b682f42c6eee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834900 train_samples/triplets.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l train_samples/triplets.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4b1sRbtRDLj1"
   },
   "source": [
    "## 模型构建与训练\n",
    "#### \\[稀牛学院 x 网易云课程\\]《深度学习工程师(实战)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "zXn2mOUGtUJv",
    "outputId": "f7b95bc8-0610-432d-86f0-9c0243968f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_22 (None, None, None, 3)\n",
      "block1_conv1 (None, None, None, 64)\n",
      "block1_conv2 (None, None, None, 64)\n",
      "block1_pool (None, None, None, 64)\n",
      "block2_conv1 (None, None, None, 128)\n",
      "block2_conv2 (None, None, None, 128)\n",
      "block2_pool (None, None, None, 128)\n",
      "block3_conv1 (None, None, None, 256)\n",
      "block3_conv2 (None, None, None, 256)\n",
      "block3_conv3 (None, None, None, 256)\n",
      "block3_pool (None, None, None, 256)\n",
      "block4_conv1 (None, None, None, 512)\n",
      "block4_conv2 (None, None, None, 512)\n",
      "block4_conv3 (None, None, None, 512)\n",
      "block4_pool (None, None, None, 512)\n",
      "block5_conv1 (None, None, None, 512)\n",
      "block5_conv2 (None, None, None, 512)\n",
      "block5_conv3 (None, None, None, 512)\n",
      "block5_pool (None, None, None, 512)\n",
      "input_23 (None, 224, 224, 3)\n",
      "input_24 (None, 224, 224, 3)\n",
      "global_average_pooling2d_8 (None, 512)\n",
      "conv2d_15 (None, 14, 14, 96)\n",
      "conv2d_16 (None, 7, 7, 96)\n",
      "dense_22 (None, 4096)\n",
      "max_pooling2d_15 (None, 4, 4, 96)\n",
      "max_pooling2d_16 (None, 4, 4, 96)\n",
      "dropout_15 (None, 4096)\n",
      "flatten_15 (None, 1536)\n",
      "flatten_16 (None, 1536)\n",
      "dense_23 (None, 4096)\n",
      "lambda_30 (None, 1536)\n",
      "lambda_31 (None, 1536)\n",
      "dropout_16 (None, 4096)\n",
      "concatenate_15 (None, 3072)\n",
      "lambda_29 (None, 4096)\n",
      "concatenate_16 (None, 7168)\n",
      "dense_24 (None, 4096)\n",
      "lambda_32 (None, 4096)\n",
      "Found 8349 images belonging to 201 classes.\n"
     ]
    }
   ],
   "source": [
    "from  __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from ImageDataGeneratorCustom import ImageDataGeneratorCustom\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "def convnet_model_():\n",
    "    vgg_model = VGG16(weights=None, include_top=False)\n",
    "    x = vgg_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    x = Lambda(lambda  x_: K.l2_normalize(x,axis=1))(x)\n",
    "    convnet_model = Model(inputs=vgg_model.input, outputs=x)\n",
    "    return convnet_model\n",
    "\n",
    "def deep_rank_model():\n",
    " \n",
    "    convnet_model = convnet_model_()\n",
    "    first_input = Input(shape=(224,224,3))\n",
    "    first_conv = Conv2D(96, kernel_size=(8, 8),strides=(16,16), padding='same')(first_input)\n",
    "    first_max = MaxPool2D(pool_size=(3,3),strides = (4,4),padding='same')(first_conv)\n",
    "    first_max = Flatten()(first_max)\n",
    "    first_max = Lambda(lambda  x: K.l2_normalize(x,axis=1))(first_max)\n",
    "\n",
    "    second_input = Input(shape=(224,224,3))\n",
    "    second_conv = Conv2D(96, kernel_size=(8, 8),strides=(32,32), padding='same')(second_input)\n",
    "    second_max = MaxPool2D(pool_size=(7,7),strides = (2,2),padding='same')(second_conv)\n",
    "    second_max = Flatten()(second_max)\n",
    "    second_max = Lambda(lambda  x: K.l2_normalize(x,axis=1))(second_max)\n",
    "\n",
    "    merge_one = concatenate([first_max, second_max])\n",
    "\n",
    "    merge_two = concatenate([merge_one, convnet_model.output])\n",
    "    emb = Dense(4096)(merge_two)\n",
    "    l2_norm_final = Lambda(lambda  x: K.l2_normalize(x,axis=1))(emb)\n",
    "\n",
    "    final_model = Model(inputs=[first_input, second_input, convnet_model.input], outputs=l2_norm_final)\n",
    "\n",
    "    return final_model\n",
    "\n",
    "\n",
    "deep_rank_model = deep_rank_model()\n",
    "\n",
    "for layer in deep_rank_model.layers:\n",
    "    print (layer.name, layer.output_shape)\n",
    "\n",
    "model_path = \"./deep_ranking\"\n",
    "\n",
    "class DataGenerator(object):\n",
    "    def __init__(self, params, target_size=(224, 224)):\n",
    "        self.params = params\n",
    "        self.target_size = target_size\n",
    "        self.idg = ImageDataGeneratorCustom(**params)\n",
    "\n",
    "    def get_train_generator(self, batch_size):\n",
    "        return self.idg.flow_from_directory(\"./triplet\",\n",
    "                                            batch_size=batch_size,\n",
    "                                            target_size=self.target_size,\n",
    "                                            shuffle=False,\n",
    "                                            triplet_path  ='./train_samples/triplets.txt'\n",
    "                                           )\n",
    "\n",
    "    def get_test_generator(self, batch_size):\n",
    "        return self.idg.flow_from_directory(\"./triplet\",\n",
    "                                            batch_size=batch_size,\n",
    "                                            target_size=self.target_size, \n",
    "                                            shuffle=False,\n",
    "                                            triplet_path  ='./train_samples/triplets.txt'\n",
    "                                        )\n",
    "\n",
    "\n",
    "\n",
    "dg = DataGenerator({\n",
    "    \"rescale\": 1. / 255,\n",
    "    \"horizontal_flip\": True,\n",
    "    \"vertical_flip\": True,\n",
    "    \"zoom_range\": 0.2,\n",
    "    \"shear_range\": 0.2,\n",
    "    \"rotation_range\": 30,\n",
    "\"fill_mode\": 'nearest' \n",
    "}, target_size=(224, 224))\n",
    "\n",
    "batch_size = 8 \n",
    "batch_size *= 3\n",
    "train_generator = dg.get_train_generator(batch_size)\n",
    "\n",
    "\n",
    "_EPSILON = K.epsilon()\n",
    "def _loss_tensor(y_true, y_pred):\n",
    "    y_pred = K.clip(y_pred, _EPSILON, 1.0-_EPSILON)\n",
    "    loss =  tf.convert_to_tensor(0,dtype=tf.float32)\n",
    "    g = tf.constant(1.0, shape=[1], dtype=tf.float32)\n",
    "    for i in range(0,batch_size,3):\n",
    "        try:\n",
    "            q_embedding = y_pred[i+0]\n",
    "            p_embedding =  y_pred[i+1]\n",
    "            n_embedding = y_pred[i+2]\n",
    "            D_q_p =  K.sqrt(K.sum((q_embedding - p_embedding)**2))\n",
    "            D_q_n = K.sqrt(K.sum((q_embedding - n_embedding)**2))\n",
    "            loss = (loss + g + D_q_p - D_q_n )            \n",
    "        except:\n",
    "            continue\n",
    "    loss = loss/(batch_size/3)\n",
    "    zero = tf.constant(0.0, shape=[1], dtype=tf.float32)\n",
    "    return tf.maximum(loss,zero)\n",
    "\n",
    "#deep_rank_model.load_weights('deepranking.h5')\n",
    "deep_rank_model.compile(loss=_loss_tensor, optimizer=SGD(lr=0.001, momentum=0.9, nesterov=True))\n",
    "\n",
    "\n",
    "train_steps_per_epoch = int((834900)/batch_size)\n",
    "train_epocs = 5\n",
    "deep_rank_model.fit_generator(train_generator,\n",
    "                        steps_per_epoch=train_steps_per_epoch,\n",
    "                        epochs=train_epocs,\n",
    "                        verbose=1\n",
    "                        )\n",
    "\n",
    "model_path = \"deepranking.h5\"\n",
    "deep_rank_model.save_weights(model_path)\n",
    "#f = open('deepranking.json','w')\n",
    "#f.write(deep_rank_model.to_json())\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pOkguqcMPqGN"
   },
   "source": [
    "## 预测与相似度排序\n",
    "#### \\[稀牛学院 x 网易云课程\\]《深度学习工程师(实战)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5S5_y_DKkuJ"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "    help=\"Path to the deep ranking model\")\n",
    "\n",
    "ap.add_argument(\"-i1\", \"--image1\", required=True,\n",
    "    help=\"Path to the first image\")\n",
    "\n",
    "ap.add_argument(\"-i2\", \"--image2\", required=True,\n",
    "    help=\"Path to the second image\")\n",
    "\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "if not os.path.exists(args['model']):\n",
    "    print \"The model path doesn't exist!\"\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(args['image1']):\n",
    "    print \"The image 1 path doesn't exist!\"\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(args['image2']):\n",
    "    print \"The image 2 path doesn't exist!\"\n",
    "    exit()\n",
    "\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from skimage import transform\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Embedding\n",
    "\n",
    "def convnet_model_():\n",
    "    vgg_model = VGG16(weights=None, include_top=False)\n",
    "    x = vgg_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    x = Lambda(lambda  x_: K.l2_normalize(x,axis=1))(x)\n",
    "    convnet_model = Model(inputs=vgg_model.input, outputs=x)\n",
    "    return convnet_model\n",
    "\n",
    "def deep_rank_model():\n",
    " \n",
    "    convnet_model = convnet_model_()\n",
    "    first_input = Input(shape=(224,224,3))\n",
    "    first_conv = Conv2D(96, kernel_size=(8, 8),strides=(16,16), padding='same')(first_input)\n",
    "    first_max = MaxPool2D(pool_size=(3,3),strides = (4,4),padding='same')(first_conv)\n",
    "    first_max = Flatten()(first_max)\n",
    "    first_max = Lambda(lambda  x: K.l2_normalize(x,axis=1))(first_max)\n",
    "\n",
    "    second_input = Input(shape=(224,224,3))\n",
    "    second_conv = Conv2D(96, kernel_size=(8, 8),strides=(32,32), padding='same')(second_input)\n",
    "    second_max = MaxPool2D(pool_size=(7,7),strides = (2,2),padding='same')(second_conv)\n",
    "    second_max = Flatten()(second_max)\n",
    "    second_max = Lambda(lambda  x: K.l2_normalize(x,axis=1))(second_max)\n",
    "\n",
    "    merge_one = concatenate([first_max, second_max])\n",
    "\n",
    "    merge_two = concatenate([merge_one, convnet_model.output])\n",
    "    emb = Dense(4096)(merge_two)\n",
    "    l2_norm_final = Lambda(lambda  x: K.l2_normalize(x,axis=1))(emb)\n",
    "\n",
    "    final_model = Model(inputs=[first_input, second_input, convnet_model.input], outputs=l2_norm_final)\n",
    "\n",
    "    return final_model\n",
    "\n",
    "\n",
    "model = deep_rank_model()\n",
    "\n",
    "# for layer in model.layers:\n",
    "#     print (layer.name, layer.output_shape)\n",
    "\n",
    "model_path = \n",
    "image1_path = \n",
    "image2_path = \n",
    "\n",
    "model.load_weights(model_path)\n",
    "\n",
    "image1 = load_img(image1_path)\n",
    "image1 = img_to_array(image1).astype(\"float64\")\n",
    "image1 = transform.resize(image1, (224, 224))\n",
    "image1 *= 1. / 255\n",
    "image1 = np.expand_dims(image1, axis = 0)\n",
    "\n",
    "embedding1 = model.predict([image1, image1, image1])[0]\n",
    "\n",
    "image2 = load_img(image2_path)\n",
    "image2 = img_to_array(image2).astype(\"float64\")\n",
    "image2 = transform.resize(image2, (224, 224))\n",
    "image2 *= 1. / 255\n",
    "image2 = np.expand_dims(image2, axis = 0)\n",
    "\n",
    "embedding2 = model.predict([image2,image2,image2])[0]\n",
    "\n",
    "distance = sum([(embedding1[idx] - embedding2[idx])**2 for idx in range(len(embedding1))])**(0.5)\n",
    "\n",
    "print (distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/xiniu_neteasy.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "image_retrieval_deep_ranking.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
